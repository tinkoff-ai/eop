<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="author" content="Vladislav Kurenkov">
<!--   <meta name="description" content="TODO"> -->
  <meta name="keywords" content="Vladislav Kurenkov, Sergey Kolesnikov, offline, reinforcement, learning, expected online performance, expected validation performance, RL">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" type="image/x-icon" href="images/favicon.ico">
  <link rel="stylesheet" href="css/base.css">
  <title>Showing Your Offline Reinforcement Learning Work: Online Evaluation Budget Matters</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-G1BRM14B7F"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-G1BRM14B7F');
  </script>
</head>

<body>

<div class="title">
  Showing Your Offline Reinforcement Learning Work: Online Evaluation Budget Matters
</div>
<center>
<span class="venue tap_targets"><a href="https://icml.cc/Conferences/2022">ICML 2022 (Spotlight)</a></span>
<span class="tag tap_targets">
  <a href="https://proceedings.mlr.press/v162/kurenkov22a.html">Paper</a>&nbsp;
  <a href="https://github.com/tinkoff-ai/eop">Code</a>&nbsp;
  <a href="files/bib.txt">Bibtex</a>
</span>
</center>
<br>

<div class="flourish-embed flourish-bar-chart-race" data-src="visualisation/10096583"><script src="https://public.flourish.studio/resources/embed.js"></script></div>

<div class="header">Abstract</div>
<p>
  In this work, we argue for the importance of an online evaluation budget for a reliable comparison of deep offline RL algorithms. First, we delineate that the online evaluation budget is problem-dependent, where some problems allow for less but others for more. And second, we demonstrate that the preference between algorithms is budget-dependent across a diverse range of decision-making domains such as Robotics, Finance, and Energy Management. Following the points above, we suggest reporting the performance of deep offline RL algorithms under varying online evaluation budgets. To facilitate this, we propose to use a reporting tool from the NLP field, Expected Validation Performance. This technique makes it possible to reliably estimate expected maximum performance under different budgets while not requiring any additional computation beyond hyperparameter search. By employing this tool, we also show that Behavioral Cloning is often more favorable to offline RL algorithms when working within a limited budget.  
</p>

<br>

<div class="citation">
  Showing Your Offline Reinforcement Learning Work: Online Evaluation Budget Matters
</div>
<div class="authors">
  <a href="https://vkurenkov.me">Vladislav Kurenkov</a>,
  <a href="https://scitator.com/">Sergey Kolesnikov</a>
</div>
<span class="venue"><a href="https://icml.cc/Conferences/2022">ICML 2022</a></span>
<span class="tag">
  <a href="https://arxiv.org/abs/2110.04156">Paper</a>&nbsp;
  <a href="https://github.com/tinkoff-ai/eop">Code</a>&nbsp;
  <a href="files/bib.txt">Bibtex</a>
</span>

</body>
</html>
